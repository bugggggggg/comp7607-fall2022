{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4A-T0z5AU6"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.out** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **run your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEomoMzH5Nf6"
      },
      "source": [
        "## 1 $n$-gram Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ElrINWW7oF7"
      },
      "source": [
        "### 1.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eawcuVV19kZm"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q-rNT_QL8Dvt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "have 46310 tokens.\n",
            "after add unknown token, have 20663 tokens.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "MINIMAL_FREQUENCY = 3\n",
        "START_TOKEN = '<s>'\n",
        "END_TOKEN = '</s>'\n",
        "UNKNOWN_TOKEN = '<UNK>'\n",
        "\n",
        "FILE_PATH = 'data/lm'\n",
        "TEST_FILENAME = f'{FILE_PATH}/test.txt'\n",
        "DEV_FILENAME = f'{FILE_PATH}/dev.txt'\n",
        "TRAIN_FILENAME = f'{FILE_PATH}/train.txt'\n",
        "\n",
        "word_frequency = {}\n",
        "\n",
        "def load_file(file_name):\n",
        "    texts = []\n",
        "    with open(file_name, encoding='utf-8') as f:\n",
        "        for line in f.readlines():\n",
        "            text = line.strip().lower().split(' ')\n",
        "            text.insert(0, START_TOKEN)\n",
        "            text.append(END_TOKEN)\n",
        "            texts.append(text)\n",
        "    return texts\n",
        "\n",
        "def cal_frequency(texts):\n",
        "    global word_frequency\n",
        "    for text in texts:\n",
        "        for word in text:\n",
        "            if word not in word_frequency:\n",
        "                word_frequency[word] = 1\n",
        "            else:\n",
        "                word_frequency[word] += 1\n",
        "\n",
        "def add_unk():\n",
        "    global word_frequency\n",
        "    tmp = {}\n",
        "    tmp[UNKNOWN_TOKEN] = 0\n",
        "    for key in word_frequency:\n",
        "        value = word_frequency[key]\n",
        "        if value < MINIMAL_FREQUENCY:\n",
        "            tmp[UNKNOWN_TOKEN] += value\n",
        "        else:\n",
        "            tmp[key] = value\n",
        "    word_frequency = tmp\n",
        "\n",
        "def tackle_unk(texts):\n",
        "    global word_frequency\n",
        "    for text in texts:\n",
        "        for i in range(len(text)):\n",
        "            if text[i] not in word_frequency:\n",
        "                text[i] = UNKNOWN_TOKEN\n",
        "\n",
        "train_texts = load_file(TRAIN_FILENAME)\n",
        "dev_texts = load_file(DEV_FILENAME)\n",
        "test_texts = load_file(TEST_FILENAME)\n",
        "\n",
        "cal_frequency(train_texts)\n",
        "print(f'have {len(word_frequency)} tokens.')\n",
        "\n",
        "add_unk()\n",
        "print(f'after add unknown token, have {len(word_frequency)} tokens.')\n",
        "\n",
        "tackle_unk(train_texts)\n",
        "tackle_unk(dev_texts)\n",
        "tackle_unk(test_texts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7oBATsX8uHb"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6VpAkS9odh"
      },
      "source": [
        "There are 46310 tokens in the training dataset. After removing tokens that occur less than three times, there are 20663 tokens.\n",
        "Use V as the size of vocabulary. Then there are $V^{n}$ parameters in n-gram models. Most of the parameters are zero. The number of none-zero parameters is linear to the total length of the training dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ2BGUig8TqH"
      },
      "source": [
        "### 1.2 $n$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeyANMPe9ad_"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ACSfNZGE8Yw2"
      },
      "outputs": [],
      "source": [
        "## model\n",
        "def unigram(train_texts):\n",
        "    model = {'c_up': {}, 'c_down': {}}\n",
        "    for text in train_texts:\n",
        "        for i in range(0, len(text)):\n",
        "            word = text[i]\n",
        "            if word not in model['c_up']:\n",
        "                model['c_up'][word] = 1\n",
        "            else:\n",
        "                model['c_up'][word] += 1\n",
        "    model['N'] = sum([len(l) for l in train_texts])  ######################\n",
        "    return model\n",
        "\n",
        "def bigram(train_texts):\n",
        "    model = {'c_up': {}, 'c_down': {}}\n",
        "    for text in train_texts:\n",
        "        for word in text:\n",
        "            if word not in model['c_down']:\n",
        "                model['c_down'][word] = 1\n",
        "            else:\n",
        "                model['c_down'][word] += 1\n",
        "        for i in range(1, len(text)):\n",
        "            tmp = (text[i - 1], text[i])\n",
        "            if tmp not in model['c_up']:\n",
        "                model['c_up'][tmp] = 1\n",
        "            else:\n",
        "                model['c_up'][tmp] += 1\n",
        "    return model\n",
        "\n",
        "def trigram(train_texts):\n",
        "    model = {'c_up': {}, 'c_down': {}}\n",
        "    for text in train_texts:\n",
        "        for i in range(0, len(text)):\n",
        "            if i == 0:\n",
        "                tmp = (START_TOKEN, text[i])\n",
        "            else:\n",
        "                tmp = (text[i - 1], text[i])\n",
        "            if tmp not in model['c_down']:\n",
        "                model['c_down'][tmp] = 1\n",
        "            else:\n",
        "                model['c_down'][tmp] += 1\n",
        "        for i in range(1, len(text)):\n",
        "            if i == 1:\n",
        "                tmp = (START_TOKEN, text[i - 1], text[i])\n",
        "            else:\n",
        "                tmp = (text[i - 2], text[i - 1], text[i])\n",
        "            if tmp not in model['c_up']:\n",
        "                model['c_up'][tmp] = 1\n",
        "            else:\n",
        "                model['c_up'][tmp] += 1\n",
        "        \n",
        "    return model\n",
        "\n",
        "### model\n",
        "unigram_model = unigram(train_texts)\n",
        "bigram_model = bigram(train_texts)\n",
        "trigram_model = trigram(train_texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       unigram    bigram    trigram\n",
            "train: 549.0812082590452 61.074203599247376 8.38145802944635\n",
            "dev: 524.4927581303102 1441.3089611913326 158.77343109253673\n"
          ]
        }
      ],
      "source": [
        "### calculate ppl\n",
        "import math\n",
        "\n",
        "def cal_perplexity(model, dim, texts):\n",
        "    ret = 0.0\n",
        "    # cnt = 30\n",
        "    if dim == 1:\n",
        "        N = model['N']\n",
        "        for text in texts:\n",
        "            for i in range(1, len(text) - 1):\n",
        "                if text[i] in model['c_up']:### how to deal with words that don't exist\n",
        "                    ret += math.log2(model['c_up'][text[i]] / N)\n",
        "                else:\n",
        "                    assert(True)\n",
        "    elif dim == 2:\n",
        "        for text in texts:\n",
        "            text_ppl = 0\n",
        "            for i in range(1, len(text)):\n",
        "                up, down = 0, 0\n",
        "                up_word = (text[i - 1], text[i])\n",
        "                down_word = text[i - 1]\n",
        "                if up_word in model['c_up']:\n",
        "                    up = model['c_up'][up_word]\n",
        "                if down_word in model['c_down']:\n",
        "                    down = model['c_down'][down_word]\n",
        "                if up != 0 and down != 0:### how to deal with words that don't exist\n",
        "                    text_ppl += math.log2(up / down)\n",
        "                else:\n",
        "                    # assert(False)\n",
        "                    text_ppl = -300\n",
        "                    break\n",
        "            ret += text_ppl\n",
        "    elif dim == 3:\n",
        "        for text in texts:\n",
        "            text_ppl = 0\n",
        "            for i in range(1, len(text)):\n",
        "                up, down = 0, 0\n",
        "                if i == 1:\n",
        "                    up_word = (START_TOKEN, text[i - 1], text[i])\n",
        "                    down_word = (START_TOKEN, text[i - 1])\n",
        "                else:\n",
        "                    up_word = (text[i - 2], text[i - 1], text[i])\n",
        "                    down_word = (text[i - 2], text[i - 1])\n",
        "                if up_word in model['c_up']:\n",
        "                    up = model['c_up'][up_word]\n",
        "                if down_word in model['c_down']:\n",
        "                    down = model['c_down'][down_word]\n",
        "                if up != 0 and down != 0:### how to deal with words that don't exist\n",
        "                    text_ppl += math.log2(up / down)\n",
        "                else:\n",
        "                    text_ppl = -200\n",
        "                    break\n",
        "            # cnt -= 1\n",
        "            # print(text_ppl)\n",
        "            # if cnt < 0:\n",
        "            #     break\n",
        "            ret += text_ppl\n",
        "    else:\n",
        "        assert(True)\n",
        "\n",
        "    M = sum([len(l) for l in texts])\n",
        "    return pow(2, -ret / M)\n",
        "\n",
        "\n",
        "print('       unigram    bigram    trigram')\n",
        "\n",
        "ppl1_train = cal_perplexity(unigram_model, 1, train_texts)\n",
        "ppl2_train = cal_perplexity(bigram_model, 2, train_texts)\n",
        "ppl3_train = cal_perplexity(trigram_model, 3, train_texts)\n",
        "\n",
        "print('train:', ppl1_train, ppl2_train, ppl3_train)\n",
        "        \n",
        "ppl1_dev = cal_perplexity(unigram_model, 1, dev_texts)\n",
        "ppl2_dev = cal_perplexity(bigram_model, 2, dev_texts)\n",
        "ppl3_dev = cal_perplexity(trigram_model, 3, dev_texts)\n",
        "\n",
        "print('dev:', ppl1_dev, ppl2_dev, ppl3_dev)\n",
        "\n",
        "# ppl1_test = cal_perplexity(unigram_model, 1, test_texts)\n",
        "# ppl2_test = cal_perplexity(bigram_model, 2, test_texts)\n",
        "# ppl3_test = cal_perplexity(trigram_model, 3, test_texts)\n",
        "\n",
        "# print('test:', ppl1_test, ppl2_test, ppl3_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWC56a19TbY"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The perplexity of unigram, bigram and trigram can be seen above.\n",
        "There are some problems when calculating perplexity. If the word pair doesn't appear in the training dataset, then we will meet log zero problem, which is equal to negetive infinity. And in the calculation above, I view the log(0) as **-300** for bigram and **-200** for trigram because this is less than the mimimun value of texts that exists.\n",
        "Normally, perplexity goes down as N(N-gram) decreases, which is contrary to the result. But when N grows, there are more pairs that have not appeared in the training dataset, so perplexity grows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuLL8CH1Ua-3"
      },
      "source": [
        "### 1.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7mWQhaCUixZ"
      },
      "source": [
        "#### 1.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbHxLDmVrz6"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "93_yLu9dVr0C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "add-one smoothing on bigram\n",
            "   train          dev  \n",
            "617.8600408466235 703.3725830223763\n"
          ]
        }
      ],
      "source": [
        "def cal_perplexity_bigram_addk(model, texts, k=1):\n",
        "    V = len(word_frequency)\n",
        "    ret = 0.0\n",
        "    for text in texts:\n",
        "        text_ppl = 0\n",
        "        for i in range(1, len(text)):\n",
        "            up, down = k, V * k\n",
        "            up_word = (text[i - 1], text[i])\n",
        "            down_word = text[i - 1]\n",
        "            if up_word in model['c_up']:\n",
        "                up += model['c_up'][up_word]\n",
        "            if down_word in model['c_down']:\n",
        "                down += model['c_down'][down_word]\n",
        "            text_ppl += math.log2(up / down)\n",
        "        ret += text_ppl\n",
        "        # print(text_ppl)\n",
        "    M = sum([len(l) for l in texts])\n",
        "    return pow(2, -ret / M)\n",
        "\n",
        "print('add-one smoothing on bigram')\n",
        "\n",
        "ppl2_addone_train = cal_perplexity_bigram_addk(bigram_model, train_texts, 1)\n",
        "# print(ppl2_addone_train)\n",
        "ppl2_addone_dev = cal_perplexity_bigram_addk(bigram_model, dev_texts)\n",
        "# ppl2_addone_test = cal_perplexity_bigram_addk(bigram_model, test_texts)\n",
        "print('   train          dev  ')\n",
        "print(ppl2_addone_train, ppl2_addone_dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y1WOQtsVr0D"
      },
      "source": [
        "##### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTknh9pRVr0D"
      },
      "source": [
        "The perplexity of bigram on train and dev dataset is shown above. Compared to the result in 1.2, the ppl become more reliable because the ppl after using add-one smoothing is similiar on train and dev dataset. And the ppl on dev dataset is lower because after smoothing, there is no log(0). <br/>\n",
        "But it's strange that ppl on train dataset is larger, since intuitively I think smoothing will make our model better(ppl is lower). And my explanation is that considering every sentence's possibility, c(x1, x2) + 1 / c(x1) + |V|. For every sentence, the total value added to numerator equal the length of sentence, while |V|*len(sentence) is added to denominator, so generally the possibility is lower."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTC0qJE8VVha"
      },
      "source": [
        "##### Optional: Add-k smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3itGMOOVuNg"
      },
      "source": [
        "###### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jhcuJWo7VuNg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k = 2 :  896.1034059620703 966.4312619206362\n",
            "k = 3 :  1114.562826222555 1173.4343263736675\n",
            "k = 4 :  1300.0676609064171 1349.767958113951\n"
          ]
        }
      ],
      "source": [
        "for w in range(2, 5):\n",
        "    ppl2_addk_train = cal_perplexity_bigram_addk(bigram_model, train_texts, k = w)\n",
        "    ppl2_addk_dev = cal_perplexity_bigram_addk(bigram_model, dev_texts, k = w)\n",
        "    # ppl2_addk_test = cal_perplexity_bigram_addk(bigram_model, test_texts, k = w)\n",
        "\n",
        "    print('k =', w, ': ', ppl2_addk_train, ppl2_addk_dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzpU_p6CVuNg"
      },
      "source": [
        "###### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIOUpNXYVuNh"
      },
      "source": [
        "The ppl is larger when k is increasing. Again, my explanation is that now the possibility is c(x1, x2) + k / c(x1) + k*|V|. As k gets larger, the possibility gets smaller, so ppl gets larger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFWxsN-Uj0Y"
      },
      "source": [
        "#### 1.3.2 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk11EpboWVCH"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K4N_XuN6WVCQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best parameters: [0.5, 0.4, 0.1]\n",
            "best ppl: 426.28508588292203\n"
          ]
        }
      ],
      "source": [
        "def cal_perplexity_linear(unigram, bigram, trigram, w, texts):\n",
        "    ret = 0.0\n",
        "    N = unigram['N']\n",
        "    V = len(word_frequency)\n",
        "    for text in texts:\n",
        "        text_ppl = 0\n",
        "        for i in range(1, len(text)):\n",
        "            ### trigram\n",
        "            up, down = 1, V\n",
        "            if i == 1:\n",
        "                up_word = (START_TOKEN, text[i - 1], text[i])\n",
        "                down_word = (START_TOKEN, text[i - 1])\n",
        "            else:\n",
        "                up_word = (text[i - 2], text[i - 1], text[i])\n",
        "                down_word = (text[i - 2], text[i - 1])\n",
        "            if up_word in trigram['c_up']:\n",
        "                up += trigram['c_up'][up_word]\n",
        "            if down_word in trigram['c_down']:\n",
        "                down += trigram['c_down'][down_word]\n",
        "            p3 = up / down\n",
        "\n",
        "            ### unigram\n",
        "            p1 = unigram['c_up'][text[i]] / N\n",
        "\n",
        "            ### bigram\n",
        "            up, down = 1, V\n",
        "            up_word = (text[i - 1], text[i])\n",
        "            down_word = text[i - 1]\n",
        "            if up_word in bigram['c_up']:\n",
        "                up += bigram['c_up'][up_word]\n",
        "            if down_word in bigram['c_down']:\n",
        "                down += bigram['c_down'][down_word]\n",
        "            p2 = up / down\n",
        "\n",
        "            text_ppl += math.log2(w[0] * p1 + w[1] * p2 + w[2] * p3)\n",
        "        # print(text_ppl)\n",
        "        ret += text_ppl\n",
        "    M = sum([len(l) for l in texts])\n",
        "    return pow(2, -ret / M)\n",
        "\n",
        "mn_ppl = 1e9\n",
        "best_w = []\n",
        "\n",
        "for i in range(0, 10):\n",
        "    for j in range(0, 10):\n",
        "        if i + j < 10:\n",
        "            k = 10 - i - j\n",
        "            w = [i/10, j/10, k/10]\n",
        "            ppl = cal_perplexity_linear(unigram_model, bigram_model, trigram_model, w, dev_texts)\n",
        "            if ppl < mn_ppl:\n",
        "                mn_ppl = ppl\n",
        "                best_w = w\n",
        "print(f'best parameters: {best_w}')\n",
        "print(f'best ppl: {mn_ppl}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.5, 0.4, 0.1] : 425.3966712794675\n"
          ]
        }
      ],
      "source": [
        "### use the best parameter on test dataset\n",
        "best_w = [0.5, 0.4, 0.1]\n",
        "print(best_w, ':', cal_perplexity_linear(unigram_model, bigram_model, trigram_model, best_w, test_texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh8v2P36WVCQ"
      },
      "source": [
        "##### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGkf6IV0WVCQ"
      },
      "source": [
        "I tried several k parameters, among which [0.5, 0.4, 0.1] is the best. And I use it in the test dataset and get the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvgTZcNFVato"
      },
      "source": [
        "##### Optional: Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKnXu98hWcfu"
      },
      "source": [
        "###### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKo4ZLASWcfu"
      },
      "source": [
        "First the problem can be viewed as select three parameters [w0, w1, w2] that minimize perplexity. The first idea is that we can assume value's step is 0.1 so we can iterate all possible tuples and get the best parameters.\n",
        "Another idea is what called Simulated annealing(SA), or just use binary search(BS). We can fix two parameters and do SA or BS on the third parameters. WE do this procedure three times on every w."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E8xYXuCUoAR"
      },
      "source": [
        "## 2 Preposition Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy = 0.7061\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def load_file_labels(file_name):\n",
        "    texts = []\n",
        "    with open(file_name, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            text = line.strip().lower().split(' ')\n",
        "            texts.extend(text)\n",
        "    return texts\n",
        "\n",
        "PREP = ['at', 'in', 'of', 'for', 'on']\n",
        "\n",
        "def predict(unigram, bigram, trigram, w, texts):\n",
        "    ret = 0.0\n",
        "    N = unigram['N']\n",
        "    V = len(word_frequency)\n",
        "    ret = []\n",
        "    for text in texts:\n",
        "        tmp = []\n",
        "        for i in range(1, len(text)):\n",
        "            if text[i] == '<prep>':\n",
        "                max = -1e9\n",
        "                select_prep = ''\n",
        "                for prep in PREP:\n",
        "                    text[i] = prep\n",
        "\n",
        "                    ### trigram\n",
        "                    up, down = 1, V\n",
        "                    if i == 1:\n",
        "                        up_word = (START_TOKEN, text[i - 1], text[i])\n",
        "                        down_word = (START_TOKEN, text[i - 1])\n",
        "                    else:\n",
        "                        up_word = (text[i - 2], text[i - 1], text[i])\n",
        "                        down_word = (text[i - 2], text[i - 1])\n",
        "                    if up_word in trigram['c_up']:\n",
        "                        up += trigram['c_up'][up_word]\n",
        "                    if down_word in trigram['c_down']:\n",
        "                        down += trigram['c_down'][down_word]\n",
        "                    p3 = up / down\n",
        "\n",
        "                    ### unigram\n",
        "                    p1 = unigram['c_up'][text[i]] / N\n",
        "\n",
        "                    ### bigram\n",
        "                    up, down = 1, V\n",
        "                    up_word = (text[i - 1], text[i])\n",
        "                    down_word = text[i - 1]\n",
        "                    if up_word in bigram['c_up']:\n",
        "                        up += bigram['c_up'][up_word]\n",
        "                    if down_word in bigram['c_down']:\n",
        "                        down += bigram['c_down'][down_word]\n",
        "                    p2 = up / down\n",
        "                    text_ppl = w[0] * p1 + w[1] * p2 + w[2] * p3\n",
        "                    if text_ppl > max:\n",
        "                        max = text_ppl\n",
        "                        select_prep = prep\n",
        "                text[i] = select_prep\n",
        "                tmp.append(select_prep)\n",
        "        ret.append(tmp)\n",
        "    return ret\n",
        "\n",
        "data_inputs = load_file('data/prep/dev.in')\n",
        "data_labels = load_file_labels('data/prep/dev.out')\n",
        "model_out = predict(unigram_model, bigram_model, trigram_model, [0.0, 0.02, 0.98], data_inputs)\n",
        "ans = []\n",
        "for x in model_out:\n",
        "    ans.extend(x)\n",
        "acc = 0\n",
        "for i in range(len(ans)):\n",
        "    if ans[i] == data_labels[i]:\n",
        "        acc += 1\n",
        "print(f'accuracy = {acc / len(ans):.4f}')\n",
        "\n",
        "\n",
        "### produce test.out\n",
        "test_inputs = load_file('data/prep/test.in')\n",
        "test_labels = predict(unigram_model, bigram_model, trigram_model, [0.0, 0.02, 0.98], test_inputs)\n",
        "\n",
        "with open('test.out', 'w', encoding='utf-8') as f:\n",
        "    for prep in test_labels:\n",
        "        f.write(' '.join(prep))\n",
        "        f.write('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The accuracy on the dev dataset is 70.61%. I use the Linear Interpolation method and manually choose the parameter which is different from the best parameter for perplexity. My explanation is that it is somewhat ridiculous to use unigram to predict prepostion. Thus I only focus on bigram and trigram. And I manually try some parameters and get the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is my previous work because I didn't realize that we shall use the model in section1 to deal with section2. And I tried to use something learnt from the class to do it. I think it's bad to just delete it so I keep it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I use double LSTM. First we reorder the dataset, with duplicated sentence. Every sentence now only deal with one preposition. And I ensure the prepositon located in the mid of the sentence by add '<pad>' in the begining and at the end. And I divided the dev dataset into 90% and 10% which used for training and validation. The the model goes into the double LSTM network. <br/>\n",
        "The network first use word embedding to represent the word. Then, because for every sentence, we only need to predict the mid prepostion, I use two LSTMs. One goes from begining to the mid and the other goes from the end to the mid. I then concatanate the result of the two LSTMs and goes into a linear layer to pridict the preposition.<br/>\n",
        "By choosing proper hyperparameters, the model can get 66% accuracy on the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finish loading ----------------------------------------------------------------------------------------------------\n",
            "test_maxlen: 86 ----------------------------------------------------------------------------------------------------\n",
            "max_len: 86 ----------------------------------------------------------------------------------------------------\n",
            "finish data ----------------------------------------------------------------------------------------------------\n",
            "start train ----------------------------------------------------------------------------------------------------\n",
            "time1.09, epoch_0: dev_accuracy 64.06, loss 1.231169880964817\n",
            "time0.74, epoch_1: dev_accuracy 62.75, loss 0.41890043860826737\n",
            "time1.04, epoch_2: dev_accuracy 63.50, loss 0.09614764583798555\n",
            "time0.86, epoch_3: dev_accuracy 64.62, loss 0.05361938773869322\n",
            "time0.93, epoch_4: dev_accuracy 67.81, loss 0.030104539691446684\n",
            "time0.68, epoch_5: dev_accuracy 64.06, loss 0.01125405211515056\n",
            "time0.65, epoch_6: dev_accuracy 65.31, loss 0.007060645544417919\n",
            "time0.70, epoch_7: dev_accuracy 64.62, loss 0.003996676507179076\n",
            "time0.81, epoch_8: dev_accuracy 65.00, loss 0.0018920139521539498\n",
            "time0.70, epoch_9: dev_accuracy 64.69, loss 0.0008936141843710524\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "\n",
        "import copy\n",
        "import random\n",
        "import time\n",
        "\n",
        "from urllib3 import encode_multipart_formdata\n",
        "\n",
        "def message(msg):\n",
        "    print(msg, '-'*100)\n",
        "\n",
        "def set_seed(seed=1120):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "### const\n",
        "PREP_LABEL = '<prep>'\n",
        "UNKNOWN_LABEL = '<unk>'\n",
        "PAD_LABEL = '<pad>'\n",
        "PREP = ['at', 'in', 'of', 'for', 'on']\n",
        "PREP_MAP = {'at': 0, 'in': 1, 'of': 2, 'for': 3, 'on':4}\n",
        "CLASSES_NUM = len(PREP)\n",
        "MAX_LEN = 0\n",
        "\n",
        "### hyperparameter\n",
        "SHUFFLE_DATASET = True\n",
        "BATCH_SIZE = 64\n",
        "WORD_DIM = 512\n",
        "LEARNING_RATE=0.01\n",
        "EPOCH = 10\n",
        "\n",
        "\n",
        "def load_file_inputs(file_name):\n",
        "    texts = []\n",
        "    cnts = []\n",
        "    with open(file_name, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            text = line.strip().lower().split(' ')\n",
        "            cnt = 0\n",
        "            for i, word in enumerate(text):\n",
        "                if word == PREP_LABEL.lower():\n",
        "                    text[i] = UNKNOWN_LABEL\n",
        "                    cnt += 1\n",
        "            cnts.append(cnt)\n",
        "            for i, word in enumerate(text):\n",
        "                if word == UNKNOWN_LABEL:\n",
        "                    tmp = copy.deepcopy(text)\n",
        "                    tmp[i] = PREP_LABEL\n",
        "                    texts.append(tmp)\n",
        "    return texts, cnts\n",
        "\n",
        "def load_file_labels(file_name):\n",
        "    texts = []\n",
        "    with open(file_name, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            text = line.strip().lower().split(' ')\n",
        "            texts.extend(PREP_MAP[prep] for prep in text)\n",
        "    return np.array(texts)\n",
        "\n",
        "data_inputs, _ = load_file_inputs('data/prep/dev.in')\n",
        "data_labels = load_file_labels('data/prep/dev.out')\n",
        "\n",
        "test_inputs, test_cnts = load_file_inputs('data/prep/test.in')\n",
        "\n",
        "# print(len(data_inputs))\n",
        "# print(len(data_labels))\n",
        "assert(len(data_inputs) == len(data_labels))\n",
        "\n",
        "message('finish loading')\n",
        "\n",
        "def add_pad(texts, init_maxlen=0):\n",
        "    max_len = init_maxlen\n",
        "    ret = []\n",
        "    for text in texts:\n",
        "        p = text.index(PREP_LABEL)\n",
        "        max_len = max(max_len, p, len(text) - p - 1)\n",
        "    for i, text in enumerate(texts):\n",
        "        p = text.index(PREP_LABEL)\n",
        "        l, r = p, len(text) - p - 1\n",
        "        tmp = [PAD_LABEL] * (max_len - l) + text + [PAD_LABEL] * (max_len - r)\n",
        "        tmp[max_len+1:len(tmp)] = tmp[len(tmp)-1:max_len:-1]\n",
        "        ret.append(tmp)\n",
        "    return ret, max_len\n",
        "\n",
        "def init_vocab(texts):\n",
        "    word2id = {}\n",
        "    word2id[PAD_LABEL] = 0\n",
        "    word2id[UNKNOWN_LABEL] = 1\n",
        "    word2id[PREP_LABEL] = 2\n",
        "    for text in texts:\n",
        "        for word in text:\n",
        "            if word not in word2id:\n",
        "                word2id[word] = len(word2id)\n",
        "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2id), WORD_DIM))  ## reverse the second half\n",
        "    embeddings[word2id[PAD_LABEL]] = np.zeros((WORD_DIM, )) ## <pad> is zero\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        texts[i] = [word2id[word] for word in text]\n",
        "    texts = np.array(texts)\n",
        "    return word2id, embeddings,\n",
        "\n",
        "### load test data\n",
        "test_inputs, test_maxlen = add_pad(test_inputs)\n",
        "message(f'test_maxlen: {test_maxlen}')\n",
        "\n",
        "### load train data\n",
        "data_inputs, MAX_LEN = add_pad(data_inputs, test_maxlen)\n",
        "word2id, embeddings = init_vocab(data_inputs)\n",
        "message(f'max_len: {MAX_LEN}')\n",
        "\n",
        "for i, text in enumerate(test_inputs):\n",
        "    for j, word in enumerate(text):\n",
        "        if word in word2id:\n",
        "            test_inputs[i][j] = word2id[word]\n",
        "        else:\n",
        "            test_inputs[i][j] = word2id[UNKNOWN_LABEL]\n",
        "\n",
        "\n",
        "### split input into train and dev \n",
        "def data_loader(inputs_train, inputs_dev, labels_train, labels_dev, batch_size=BATCH_SIZE):\n",
        "    # print(inputs_train)\n",
        "    inputs_train = torch.tensor(inputs_train)\n",
        "    inputs_dev = torch.tensor(inputs_dev)\n",
        "    labels_train = torch.tensor(labels_train, dtype=torch.long)\n",
        "    labels_dev = torch.tensor(labels_dev, dtype=torch.long)\n",
        "\n",
        "    data_train = TensorDataset(inputs_train, labels_train)\n",
        "    sampler_train = RandomSampler(data_train)\n",
        "    dataloader_train = DataLoader(data_train, sampler=sampler_train, batch_size=batch_size)\n",
        "\n",
        "    data_dev = TensorDataset(inputs_dev, labels_dev)\n",
        "    sampler_dev = SequentialSampler(data_dev)\n",
        "    dataloader_dev = DataLoader(data_dev, sampler=sampler_dev, batch_size=batch_size)\n",
        "\n",
        "    return dataloader_train, dataloader_dev\n",
        "\n",
        "\n",
        "inputs_train, inputs_dev, labels_train, labels_dev = train_test_split(\\\n",
        "    data_inputs, data_labels, test_size=0.1, shuffle=SHUFFLE_DATASET)\n",
        "\n",
        "dataloader_train, dataloader_dev = data_loader(inputs_train, inputs_dev, labels_train, labels_dev, BATCH_SIZE)\n",
        "\n",
        "message('finish data')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### model #######################################################################\n",
        "HIDDEN_LAYER_SIZE = 256\n",
        "vocab_size = len(word2id)\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, \n",
        "                word_dim=WORD_DIM):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=word_dim, hidden_size=HIDDEN_LAYER_SIZE, num_layers=1, batch_first=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output, (h_n, c_n) = self.lstm(input)\n",
        "        # print(h_n)\n",
        "        # print(output.shape)\n",
        "        return h_n[-1]\n",
        "        # return output[:,-1,:]\n",
        "\n",
        "class Double_LSTM(nn.Module):\n",
        "    def __init__(self, \n",
        "                vocab_size,\n",
        "                word_dim=WORD_DIM,\n",
        "                pretrained_embedding=None,\n",
        "                classes_num=CLASSES_NUM,\n",
        "                dropout=0.5):\n",
        "        super(Double_LSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                        embedding_dim=word_dim,\n",
        "                                        max_norm=5.0,\n",
        "                                        padding_idx=0)\n",
        "        self.lstm1 = LSTM(word_dim)\n",
        "        self.lstm2 = LSTM(word_dim)\n",
        "        self.fc = nn.Linear(2 * HIDDEN_LAYER_SIZE, classes_num)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.embedding(input)\n",
        "        # print(x)\n",
        "        xl, _, xr = x.split([MAX_LEN, 1, MAX_LEN], dim=1)\n",
        "        xl = self.lstm1(xl)\n",
        "        xr = self.lstm2(xr)\n",
        "        # print(xl)\n",
        "        x_fc = torch.cat([xl, xr], dim=1)\n",
        "        x_fc = self.dropout(x_fc)\n",
        "        # print(x_fc.shape)\n",
        "        # x_fc = self.dropout(x_fc)\n",
        "        x_fc = self.fc(x_fc)\n",
        "        return x_fc\n",
        "\n",
        "\n",
        "### train ############################################################################\n",
        "message('start train')\n",
        "\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "def train(model, optimizer, dataloader_train, dataloader_dev, epochs=EPOCH):\n",
        "    best_accuracy = 0\n",
        "    for epoch_i in range(epochs):\n",
        "        start_time = time.time()\n",
        "        total_error = 0\n",
        "\n",
        "        model.train()\n",
        "        for step, batch in enumerate(dataloader_train):\n",
        "            inputs, labels = tuple(t.to(device) for t in batch)\n",
        "            model.zero_grad()\n",
        "            y = model(inputs)\n",
        "            # print(y)\n",
        "            # exit()\n",
        "            error = loss_f(y, labels)\n",
        "            total_error += error.item()\n",
        "            # print(error)\n",
        "\n",
        "            error.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        dev_accuracy = evaluate(model, dataloader_dev)\n",
        "        avg_error = total_error / len(dataloader_train)\n",
        "\n",
        "        if dev_accuracy > best_accuracy:\n",
        "            best_accuracy = dev_accuracy\n",
        "            torch.save(model.state_dict(), 'double_lstm.pt')\n",
        "        \n",
        "        time_elapsed = time.time() - start_time\n",
        "        print(f'time{time_elapsed:.2f}, epoch_{epoch_i}: dev_accuracy {dev_accuracy:.2f}, loss {avg_error}')\n",
        "\n",
        "def evaluate(model, dataloader_dev):\n",
        "    model.eval()\n",
        "    accuracy_list = []\n",
        "    print_first = True\n",
        "    for step, batch in enumerate(dataloader_dev):\n",
        "        inputs, labels = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            y = model(inputs)\n",
        "        preds = torch.argmax(y, dim=1).flatten()\n",
        "        accuracy = (preds == labels).cpu().numpy().mean() * 100\n",
        "        accuracy_list.append(accuracy)\n",
        "        # if print_first:\n",
        "        #     print(f'labels{labels}')\n",
        "        #     print(f'predic{preds}')\n",
        "        #     print_first = False\n",
        "        # exit(0)\n",
        "\n",
        "    return np.mean(accuracy_list)\n",
        "\n",
        "\n",
        "double_lstm = Double_LSTM(len(word2id))\n",
        "double_lstm.to(device)\n",
        "\n",
        "# optimizer = optim.Adadelta(double_lstm.parameters(),  ## get all 2 if use Adadelta\n",
        "#                                 lr=LEARNING_RATE,\n",
        "#                                 rho=0.95)\n",
        "optimizer = optim.Adam(double_lstm.parameters(), \n",
        "                        lr=LEARNING_RATE)\n",
        "\n",
        "train(double_lstm, optimizer, dataloader_train, dataloader_dev)\n",
        "# exit()\n",
        "\n",
        "\n",
        "\n",
        "# ### model on test data ##########################################################\n",
        "# double_lstm.load_state_dict(torch.load('double_lstm.pt'))\n",
        "# test_inputs = torch.tensor(test_inputs)\n",
        "# # for i in range(len(test_inputs)):\n",
        "# #     test_inputs[i].to(device)\n",
        "# # test_inputs.to(device)\n",
        "\n",
        "# test_labels = double_lstm(test_inputs)\n",
        "# test_labels = torch.argmax(test_labels, dim=1).flatten()\n",
        "\n",
        "# preps = []\n",
        "# p = 0\n",
        "# for cnt in test_cnts:\n",
        "#     tmp = []\n",
        "#     s = p\n",
        "#     while p < s + cnt:\n",
        "#         tmp.append(PREP[test_labels[p].item()])\n",
        "#         p += 1\n",
        "#     preps.append(tmp)\n",
        "\n",
        "# with open('dblstm_test.out', 'w', encoding='utf-8') as f:\n",
        "#     for prep in preps:\n",
        "#         f.write(' '.join(prep))\n",
        "#         f.write('\\n')\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('ml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "d559f30ae9ab779927f3cb792f1cac9b725f5061f38755c4ef102b55af6275f0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
